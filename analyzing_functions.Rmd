---
title: "Functions and models in R"
output: 
  html_document:
    highlight: pygments
    theme: sandstone
    toc: true
    toc_float: true
    bibliography: "../bibliography.bib"   
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

# Overview

The goal of this lab is to visit the model functions created by R - to begin to learn how to build them, how to work with them, and how to compare different models. 

# Revisiting `lm`

The template for most of the model functions in R is the linear model, `lm`. Let's revisit the `Lily_sum` data from last week's lecture. We'll restrict our attention to columns 1 to 8. 
```{r}
library(emdbook)
lilies <- Lily_sum[,1:8]
summary(lilies)
```


We'll start with a simple one variable linear regression.
```{r}
lilymod1 <- lm(flowers~vegetative+gopher, data=lilies)
```
The result of running the `lm` function, which we have named `lilymod1`, contains quite a lot of information. To get a list of its components, we can use the `names` command.
```{r}
names(lilymod1)
```
Many of these you probably won't ever use, but `coefficients`, `residuals` and `fitted.values` are quite useful. To access to coefficients of the model, we can use `lilymod1$coefficients` or `coefficients(lilymod1)`, with the latter generally being considered better practice. (It makes no difference in this situation, but there are potential scenarios where using `$coefficients` instead of `coefficients()` wouldn't give you what you expect.) You can type `coef()`  instead of `coefficients()`. Similarly, get the residuals with `residuals()` or `resid()` and the fitted values (predictions) with `fitted.values()` or `fitted()`. 

For some tasks, it is useful to add the residuals or the predictions to the data frame use to build the model. To add residuals as the variable `resid1`, enter
```{r}
lilies$resid1 <- resid(lilymod1)
```

(@) Add `pred1` to `lilies` using `fitted()` and make a plot showing the data and the predictions with the following code:
```{r, echo=FALSE}
lilies$pred1 <- fitted(lilymod1)
```
```{r, eval=FALSE}
ggplot(lilies, aes(vegetative))+
  geom_point(aes(y=flowers))+
  geom_point(aes(y=pred1, color="red"), show.legend = FALSE)
```
Based on this plot, do you expect that the coefficient of `gopher` in `lilymod1` is positive or negative? Why?


It is important to note that **adding model output to data only works if there are no missing values**. 

## More information about a model

Usually, the first thing one does after creating any sort of model is take a look at the `summary`. 
```{r}
summary(lilymod1)
```
This gives us some diagnostic information, such as $R^2$ and standard errors in the coefficients. The added information about the coefficients can be accessed using row and column notation. For example, to get the standard error in the estimate for the intercept:
```{r}
coef(summary(lilymod1))[1,2]
```

(@) Use the `coef()` command to calculate the upper and lower bounds of a 95% confidence interval for the `vegetative` coefficient. This task can also be accomplished using the `confint()` command. Use this to check your answer.

To access the statistics such as $R^2$, use the `$` notation, for example `summary(lilymod1)$fstatistic`.

## Making predictions on new data

Sometimes we want to see what a model would do with new input data. For this we will use the `predict()` function. We need to give the new data to `predict()` as a data frame where the variable names are the input variables in the model. Try the following code to compute what `lilymod1` predicts for sites with `gopher`=5 and `vegetative` between 20 and 25.
```{r, eval=FALSE}
newlily <- data.frame(vegetative = 20:25, gopher = rep(5,6))
newlily$prediction <- predict(lilymod1, newdata = newlily)
newlily
```

(@) Compute the predictions from `lilymod1` for `vegetative`=25 and `gopher` between 5 and 10.

# Transformations and link functions

A histogram of the response variable `flowers` shows significant right skew, some of which is still present in the residuals for `lilymod1`. A log transform might make sense.

```{r}
par(mfrow=c(1,2))
hist(lilies$flowers)
hist(lilies$resid1)
par(mfrow=c(1,1))
```



You can use `lm()` to fit a model for the mean of log-transformed `flowers` or `glm` to fit the log of the mean of `flowers`. We'll continue to use `vegetative` and `gopher` as predictors so that we can compare with `lilymod1`.

## Fitting to the log-transformed variable 

(@) Try to make a model with the following code. What happens? 
```{r, eval=FALSE}
lilymod2 <- lm(log(flowers)~vegetative+gopher, data = lilies)
```

There are a number of solutions to this problem, which is caused by the fact that $\log(0)$ is not defined. Before we do anything, we should check how extensive the problem is. Use `sum(lilies$flowers==0)` to count how many times `flowers` is exactly 0. One common fix to log transforming a variable with zeros is to add a small amount before transforming.
```{r}
# Put any computations done in a formula in I() to ensure they are properly interpreted. 
lilymod2 <- lm(I(log(flowers+1))~vegetative+gopher, data = lilies) 
```

(@) Compare the output of `summary(lilymod2)` to `summary(lilymod1)`. In what ways do they tell the same story? 

## Back-transforming to the original scale

To make predictions on the scale of the original data, we have to reverse the computation log(`flowers`+1) by exponentiating and then subtracting 1. Also, we first add half the variance in the residuals to correct for how the error transforms. (In practice, this step is often ignored.)

```{r}
vr <- var(resid(lilymod2))
lilies$pred2 <- exp(fitted(lilymod2)+vr/2)-1
```



## Fitting using a log link

To fit with a link function, we use `glm` instead of `lm`:
```{r}
lilymod3 <- glm(I(flowers+1)~vegetative+gopher, data = lilies, family=gaussian(link="log"))
```

We can compare the coefficients and their standard errors using `summary` on `lilymod2` and `lilymod3`. The bottom part of the `summary` output for `glm` is different though. Instead of *Residual standard error*, $R^2$, and $F$ statistics, we have *Null* and *Residual deviance* and *AIC*.   

As before, we can add the predicted values to the data. The `fitted` command for `glm` does the back transform automatically.

```{r}
lilies$pred3 <- fitted(lilymod3) - 1
```

# Model comparison

Now we have three different models describing flower counts in terms of indices of vegetation and gopher habitation. How can we determine which we prefer? The situation is similar to when we use ANOVA to compare models that do or don't include certain terms, but since the models are not nested, ANOVA does not make sense.

The ideal situation is when there is a theoretical justification for the model you choose. But if you are just getting to know the data or experimenting with different designs, there are various ways to compare models.

## Comparing the models graphically

Here is one way that the models can be compared by plotting the predictions against the actual data. We add a reference line of what would be accurate predictions.
```{r}
ggplot(lilies, aes(flowers))+
  geom_point(aes(y=pred1, color="a"))+
  geom_point(aes(y=pred2, color="b"))+
  geom_point(aes(y=pred3, color="c"))+
  geom_abline(slope = 1, intercept = 0)+
  scale_color_discrete(name = "model", labels = c("lilymod1", "lilymod2", "lilymod3"))+
  ylab("predicted flowers")
```

The predictions made by all three models appear rather similar. 

The colors in `geom_point` and the labels created in `scale_color_discrete` are a bit of a hack. As long as each of the values assigned to color is a character (text in quotes) and different in each `geom_point`, ggplot will assign the colors. To do this in a more "tidy" way, we should make a data frame of predictions by pivoting this to a long form with `prediction` and `model` as variables.

## Comparison using error metrics

Recall that regression is choosing a model that minimizes the mean squared error (MSE).
$$
MSE=\frac{1}{n}\sum_{i=1}^n(y_i-\hat{y})^2
$$
We can use this metric to compare our models. To do this, we'll write a custom command for computing the MSE from vectors of predictions and actual data. 
```{r}
mse <- function(prediction, actual){
  sum((actual - prediction)^2)/length(actual)
}
mse(lilies$pred1, lilies$flowers)
mse(lilies$pred2, lilies$flowers)
mse(lilies$pred3, lilies$flowers)
```

This metric supports the observation that the three models are similar, with `lilymod3` having the lowest mean squared error and `lilymod2` the highest, but only slightly in both cases.

Mean squared error is only one of infinitely many possible error metrics we could have chosen. A very similar option would have been mean absolute error (MAE).
$$
MAE=\frac{1}{n}\sum_{i=1}^n|y_i-\hat{y}|
$$

(@) Revise the function for mean squared error to compute mean absolute error. How do the models compare by this metric?

## Akaike information criterion (AIC)

Another metric that we can use is the Akaike information criterion, or AIC. This only allows us to compare `lilymod1` and `lilymod3`. Transforming the response variable in `lilymod2` has an effect on this metric. It is possible with some work to overcome this difficulty, but we'll leave that for another day.

```{r}
AIC(lilymod1)
AIC(lilymod3)
```

Smaller values of AIC indicate a better fit, so this metric agrees that `lilymod3` is preferred. We will revisit AIC later, but the essential computation to make is this: Given a set of models, compute their AICs. If $AIC_{min}$ is the smallest of these values and the AIC for the $i^{th}$ model is $AIC_i$ then 
$$
\exp\left(\frac{AIC_{min}-AIC_i}{2}\right)
$$
is roughly interpretable as the probability that the $i^{th}$ model is actually better than the model with the lower AIC. 

(@) What is the probability that `lilymod1` is actually better than `lilymod3` based on AIC?

# More Practice

(@) How can we interpret the coefficients of `lilymod3` in the context of the experiment? (An increase of one unit for the `vegetative` index is expected to correspond to ...)

(@) In this lab so far, we have not considered the other variables such as `moisture` and `rockiness`. Experiment with adding these variables and come up with your preferred model. Describe your model. Support your preference by comparing it to other models that you considered.





